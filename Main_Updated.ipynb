{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee9e49-f84c-4594-b341-e09ae2484eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf88b27-ce1a-4eee-87c9-466041681775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root= 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root= 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f151581-835c-47d8-8476-9902bf980573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbaa8d4-9b94-4adb-8881-8c3a37de2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e285afb-c32b-43dd-88b4-957a0bc7d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c011-d9ad-4758-b296-4fe4d0ad87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9800c7-c2e4-4884-82d6-db9909027148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33e039-23a2-4097-83ae-0e11085cf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train': DataLoader(train_data, \n",
    "                        batch_size =100,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1),\n",
    "\n",
    "    'test': DataLoader(test_data, \n",
    "                        batch_size =100,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24925fe6-8816-4244-9f7e-b18629fddb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):  # Fixed: double underscores __init__\n",
    "        super(CNN, self).__init__()  # Fixed: double underscores __init__\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)  # Fixed: conv1 (not convl)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):  # Fixed: Proper indentation (was inside __init__)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)  # Changed to log_softmax with dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5a4da-4d3f-4aba-9c10-698c93333a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)  # Fixed: This line was outside the for loop\n",
    "            test_loss += loss_fn(output, target).item() \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Fixed: Added parentheses\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd5012-89db-4751-81ef-60cba48cc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d336860-d42b-4a68-b599-7f9227ab51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d070193-5e18-4381-8483-5d3f7efe7c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "\n",
    "data,target = test_data[4] #modify this to get different results\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "prediction = output.argmax(dim =1, keepdim= True).item()\n",
    "\n",
    "print('Prediction: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31145908-58f2-4421-a9a3-0fcb691ee26a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "data, target = test_data[8]  # Replace '4' with your desired index\n",
    "data = data.unsqueeze(0).to(device)  # Add batch dimension\n",
    "output = model(data)\n",
    "prediction = output.argmax(dim=1).item()\n",
    "print(f\"Model Prediction: {prediction}, True Label: {target}\")\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1de3dff-ab27-4ff0-b420-955383273b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFD5JREFUeJzt3QuQVnX5B/DfclsRkDUIwqZIFK200qzMMMnykpUpTjrVNFja1czUrKYaNS2tkWaSqSl0MLtYll21MdEw0spqCJW8pFkJpXQRs3IlUOD85znzfx93l91pz4suLHw+M8ju4Tzv+b1nd8/3/C577KiqqioAUEoZsaUbAMDWQygAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoMGx0dHSUj3/840N+3J/+9Kf1sePvJ0q8j3hN2NoIhWHkiCOOKDvvvHP5+9//vsm//fvf/y7Tpk0r+++/f9m4cWPZUp71rGeV173udf3+229+85v6QvjlL395yNs1XL31rW8t48eP39LNYDsiFIaRL3zhC+XRRx8tp5122ib/9tGPfrSsXr26XHzxxWXECF9WoD2uHsPIrrvuWs4+++xy+eWXl+uuuy63L126tCxYsKCcfvrp5QUveMGT2obohaxdu/ZJPQaw5QiFYSYu/M9//vPLSSedVF+cN2zYUN797neX6dOn14Hxk5/8pLz85S8v48aNK11dXeWoo44qv/vd7zYZkohhnsGMc8fnJ598cvn6179e9tprr9LZ2VkWLVr0hA+P/OlPfyqHH3543e5ddtmlnHvuuWUwD/C95ZZb6mG1nXbaqX6dV73qVeVXv/pVr33++c9/ljPOOKM873nPq/eJfaNm+fLlm7zefffdV44++ui6HVOmTKl7ZevWrev32L/+9a/Lq1/96jJx4sSy4447ltmzZ5df/OIXm+z385//vLz4xS8uO+ywQ9ltt93KRRddVJ6IIbqY43jRi15Uxo4dW7+31pzH9773vfrzON5+++1Xn6Oefvvb39bnfcaMGfU+T3va08oJJ5xQHnzwwU2O1TpGz7YPNB9y2WWX1ceL9jzlKU8pb3zjG8tf/vKXzXqvDL1RW+CYbIZRo0bVQ0Qve9nLyic+8Yn6wnXzzTfXF+qbbrqpvtjFD3v84P73v/8tn/vc58qsWbPqffoLgsGIoLniiivqcJg8eXLbrzOQCLa4uL70pS8tF1xwQf1eIuDWr19fh8NA7rjjjjoA4yL/oQ99qIwePbq+aL3iFa8oN9xwQz2/EiJwfvCDH5Rjjz227m3FnEzsFxfxO++8sw6hEOcrQuXPf/5zOeWUU+rtX/va1+r33985iXMdF8FoawzZXXrppeWVr3xl+dnPflZe8pKX1Pvddttt5bDDDitPfepT669JvKfYf+rUqZt1zv7whz+UN7/5zeVd73pXectb3lI+85nPlCOPPLLuMcZQYtw0hE996lPluOOOK3fffXcOK/74xz+uz8nb3va2OhDiPMb3VPwdgdq64EeYxNcl5qrOOeec+usUX494L32dd9555cwzz6yP9fa3v7088MAD9ffeQQcdVL9O3KAwTMT/T4Hh5+STT65Gjx5djR8/vnrTm95Ub9tnn32qKVOmVA8++GDut3z58mrEiBHV3Llzc9vxxx9fTZ8+fZPXPPvss+PWvNe2+Dzq77jjjkG1K173ta99bb//tnTp0vr1Lr300l5tiW3ve9/7ctvGjRvr1xgzZkz1wAMP9GpLtLHl6KOPrvf54x//mNtWrVpVTZgwoTrooINy29q1a6sNGzb0asu9995bdXZ2Vueee25uu/DCC+tjXHHFFbntkUceqXbfffd6+5IlS7J9M2fOrA4//PD645Y1a9ZUu+66a3XooYf2auMOO+xQrVy5Mrfdeeed1ciRIzc51/2J8zNu3LhNznHU3nTTTbnt2muvrbeNHTu217EuuuiiXm1vtbOvyy+/vN7vxhtvzG1HHnlkteOOO1b3339/brvnnnuqUaNG9Wr7ihUr6vdz3nnn9XrN2267rd6373a2boaPhqm4M5s0aVJ99/fZz362/PWvfy233nprPSwQXfeWGGo69NBDy49+9KO2jxV31M997nPLkyl6IX2HrGJSffHixf3uH3etMa8SQz3RM2qJu9q4g44hm//85z/1thjyat0lR10Mk8Qw0p577ln3oFriHEX9G97whtwWw0LvfOc7ex07zvM999xTHydeKyb4488jjzxS9zRuvPHGeu4ljnXttdfWbXzmM5+Z9c95znPqobLNEV+PAw44ID9v9Yqip9LzWD17Sy0xvNMSQ5DR9uilhdb5iLbHuY+2t3pSYffdd697SD3FcFW83+gltM5F/IleyMyZM8uSJUs2670ytAwfDVMxZBIXtfjhi6GI1jh6bOsrLkJxcYqLVoyVNxVDLk+kvuPRccHueWEPe+yxR/33ihUr+n2NGJ5Ys2bNgO83LlIxnh3zIPHx/Pnz69Vb9957b33Ba4lgbVm5cmV90evbvr7HiEAIxx9//IDvMZYIx1xEDEnFhbGveM3NCeqeF/4Q8xrhGc94Rr/bH3rooV5zLDEc9M1vfrP84x//2KTdIbZH2+N89NV3W5yP6Mj19z5DDOsxfAiF7dBAvzTV82LZU887y/8lJiTjYtKfuIi39hlK559/fj3eHZOpMQ8TPakIolNPPbWt3+lo1cybN6/ss88+/e4TPZGBJqifCCNHjmy0veekfdzRx/zTBz/4wbr90dZ4TzF/0O75iO+pa665pt/j+z2L4UUobCNi9VGICcW+7rrrrnqCuNVLiF+A+9e//rXJfnGn/ES0IyZv+9NqW6utPS8qMbzR6h2E3//+9/XfA01qx2RnDO0M9H7jot+6a/7Od75TDj744HLJJZf02i/OQZyXnm2//fbb6wtoz+Dse4xYhdPqrR1yyCEDnotoYwRqq2fR37kYatFjuP766+uewllnnZXb+7YxFjBEeMeEdl99t8X5iHMWPcqeX0OGJ3MK24gYC4+7vq985Su9LvhxkYux99e85jW9fohjmCCWJrbEnMT3v//9zW5HHCeWdcZqn57irnnhwoX1xeaFL3zhJnWf//zn8+O4wMTnMewQY/T9iTvSWNVz5ZVX9hpiipVF3/jGN8qBBx5YX7Rb+/Zd3vrtb3+73H///Zu0fdWqVXWI9OzdxMqcnmLFUZzDWPHT3d3d79BW67gxdxDnIlY0tcQS4RjO2xJad/J9z8eFF164yX4ReNH2OCc9AyF6BD0dc8wx9f4RNH1fNz7vb6krWy89hW1IDGfEJGBMQJ544om5JDXGlXs+MyjWj3/4wx8uc+bMqZdexoXvi1/8Yn2X13PitR0xKfulL32pXv4ZwzX77rtvfVH41re+VQfUV7/61TJmzJheNXFHGstQY4w+JkbjonP11VfXSyv7W/7Y8slPfrJeXhkBEEswY7luLDWNAIqlrS2xpj+WUsYSzFjKG8tE4/cu+s5jvOMd76jDaO7cuWXZsmV10MaS1OiR9BS9kAi4ONcxZxGv+/SnP70OmZhUjTD64Q9/WO8bF8p4b7F0NtoYS1LjaxJ1PUN5qETbYplonJ/HHnusbnfcNMRcS1/xPRP/Fkua3/Oe99TDi3F+9t5773qyvSUCMr4WH/nIR+qAjsnpCRMm1K8ZNxrxPRG/J8IwsaWXP9G+2bNnV3vttVevbYsXL65mzZpVL03caaed6mWFsQSyr+uuu67ae++96yWde+65Z3XZZZcNuCT1ve99b6N2PfTQQ9Vpp51WL8+MZbPRjoMPPri65pprBlxyGctKDzvssHoJ5NSpU+u29F1G2ndJarj55pvrpaGxNDdq4zg9l2q2lqR+4AMfqKZNm1aflzg/v/zlL+vzF396iuWcr3/96+vXmjx5cvX+97+/WrRo0SbLOsMtt9xSHXPMMdWkSZPq5a2xVPS4446rrr/++l773XDDDdV+++1Xn+sZM2ZUCxYs6PdcN1mS2t+y3/6+VrH0NrbPmzcvt913333VnDlzqq6urmrixInVscceWy/l7e/8xnvZd99967bvtttu1cKFC+tzGcts+/rud79bHXjggXV748+zn/3suj133333/3yfbD064j9bOpjYfsUS2hiu6W8Yhq1T9ATiF936myth+DOnAAyo70qyCIJYShu/Nc62yZwCMKCYd2k9JylWp8XcU8wJxWNF2DYJBWBA8bsL8VTev/3tb/Vvhscihvi9j4F+UY3hz5wCAMmcAgBJKADQfE7B/2QcYHgbzGyBngIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBp1OMfwpNr5MiRbdWdddZZjWtOOumkxjXjxo0rQ2HFihVt1e2///6Nax5++OG2jsX2S08BgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASB1VVVVlEDo6OgazG9uJMWPGDNmD4KZOndpW3bZmzZo1jWsmTpzYuGbjxo2NaxgeBnO511MAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUA0qjHP4TBu/rqqxvXTJkypa1jrV27tnHNKaec0rjmkksuaVwzfvz4xjUrV64s7ejq6mpcs3DhwsY1J5xwQuMath16CgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkT0mlLTNnzmxcs27duraONX369MY1q1evLkOhu7u7cc1VV13V1rHmzp3buGbWrFltHYvtl54CAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkDwQj7bceuutjWuqqmrrWEP1cLuhcsghh7RV19HR0bhm2bJlbR2L7ZeeAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJA6qkE+paydh3Gx7ers7ByyB+I9+uijZWs1e/bsxjVLlixp61gbN25sXNPV1dW4pru7u3ENw8Ngfgb1FABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYA06vEPYfDWrVtXtjUnnnhi45oFCxaUobJ8+fLGNZMmTWpc44F42zc9BQCSUAAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAJJQACAJBQBSR1VVVRmEjo6OwewGW4UJEyY0rlm2bFnjmpkzZ5ahMsgf1c2uWbVqVeOaGTNmNK557LHHGteweQbz/aCnAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACQPxIP/9+lPf7pxzZw5cxrX7LzzzqUdY8eObVwzbty4xjXt/Kx3d3cPyUML2TweiAdAI0IBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGA5IF4sA2bNGlS45rVq1eXoXDUUUe1VXfVVVc94W3ZXlQeiAdAE0IBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGA5IF4QC9r165tXNPZ2dm4ZtGiRaUdRxxxRFt1FA/EA6AZoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEAa9fiHAKWMGDE094rr168fkuPQjJ4CAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAMlTUmEbNmHChMY1o0ePLkNh/vz5Q3IcmtFTACAJBQCSUAAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAFJHVVVVGYSOjo7B7AY8ScaMGdO4pru7e0geiNfOcdp5WB+bZzCXez0FAJJQACAJBQCSUAAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAII16/EO2V+eff37jmhUrVjSuufjiixvXbIvmzZvXVt3pp5/euGbEiOb3fRs2bGhcM23atMY1bJ30FABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYDUUVVVVQaho6NjMLuxhXV1dTWuueuuuxrXTJ48uXHN7bffXtpx6qmnNq7ZY489GteceeaZjWt22WWXIXlIXbu6u7uH5D09/PDDjWsYeoO53OspAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAMkD8SjnnHNO45qPfexjjWtGjhzZuGZbtH79+rbq5s+f37jmjDPOaOtYbJs8EA+ARoQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkDwllbZ0dnY2rrnyyivbOtYBBxzQuGbjxo2Na5YuXdq45oILLmhcs3jx4sY18ETwlFQAGhEKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJA/EA9hOVB6IB0ATQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEijyiBVVTXYXQEYpvQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFAAoLf8HQQh09tz2JL8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     47\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m model = \u001b[43mCNN\u001b[49m().to(device)  \u001b[38;5;66;03m# Change to CNN\u001b[39;00m\n\u001b[32m     49\u001b[39m MODEL_PATH = \u001b[33m'\u001b[39m\u001b[33mmnist_cnn.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 4. Train the model if not already saved\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'CNN' is not defined"
     ]
    }
   ],
   "source": [
    "# upload the images on jupyter manually, then run the code make sure you changed the image file name \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. Load and display your image\n",
    "img = Image.open('3.png.png').convert('L').resize((28, 28))\n",
    "\n",
    "# If your digit is white on black, uncomment the next two lines:\n",
    "# import PIL.ImageOps\n",
    "# img = PIL.ImageOps.invert(img)\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"Your Uploaded Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 2. Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "tensor_img = transform(img).unsqueeze(0)\n",
    "\n",
    "# 3. Define the simple MNIST model\n",
    "class Net(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(Net, self)._init_()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)  # Change to CNN\n",
    "MODEL_PATH = 'mnist_cnn.pth'\n",
    "\n",
    "# 4. Train the model if not already saved\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(\"Loaded pre-trained model from mnist_cnn.pth\")\n",
    "else:\n",
    "    print(\"No pre-trained model found. Training a model for 10 epochs (may take several minutes)...\")\n",
    "    import torchvision\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                                               transform=transforms.Compose([\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                               ]))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Train for 10 epochs!\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/10, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    print(\"Model trained and saved as mnist_cnn.pth!\")\n",
    "\n",
    "# 5. Predict the digit\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(tensor_img.to(device))\n",
    "    pred = output.argmax(dim=1, keepdim=True).item()\n",
    "print(f\"Predicted digit: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0af6b37-908f-44c4-8287-60331513c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained model from mnist_cnn.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Your CNN model definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Function to load and preprocess image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load image and convert to grayscale\n",
    "    img = Image.open(image_path).convert('L').resize((28, 28))\n",
    "    \n",
    "    # If your digit is white on black, uncomment the next line:\n",
    "    # img = ImageOps.invert(img)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(\"Uploaded Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Main prediction function\n",
    "def predict_digit(image_path):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CNN().to(device)\n",
    "    MODEL_PATH = 'mnist_cnn.pth'\n",
    "    \n",
    "    # Load or train model\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        print(\"Loaded pre-trained model from mnist_cnn.pth\")\n",
    "    else:\n",
    "        print(\"No pre-trained model found. Training a new model...\")\n",
    "        model = train_model(model, device)\n",
    "    \n",
    "    # Preprocess image\n",
    "    tensor_img = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_img.to(device))\n",
    "        pred = output.argmax(dim=1, keepdim=True).item()\n",
    "    \n",
    "    print(f\"Predicted digit: {pred}\")\n",
    "    #return pred\n",
    "\n",
    "#TEST HERE - change image here\n",
    "predict_digit('7.png.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7118ca7-b439-44c6-9fb1-96c69a48f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
